{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7ef20f0-722f-4240-8a79-437d4a3b8832",
   "metadata": {},
   "source": [
    "## Assignment 3: $k$ Nearest Neighbor\n",
    "\n",
    "**Do two questions.**\n",
    "\n",
    "`! git clone https://github.com/DS3001/knn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9212c0",
   "metadata": {},
   "source": [
    "**Q0.**\n",
    "1. What is the difference between regression and classification?\n",
    "2. What is a confusion table? What does it help us understand about a model's performance?\n",
    "3. What does the SSE quantify about a particular model?\n",
    "4. What are overfitting and underfitting? \n",
    "5. Why does splitting the data into training and testing sets, and choosing $k$ by evaluating accuracy or SSE on the test set, improve model performance?\n",
    "6. With classification, we can report a class label as a prediction or a probability distribution over class labels. Please explain the strengths and weaknesses of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf06c6-70fb-427d-b429-585c0ab2c3b1",
   "metadata": {},
   "source": [
    "Regression and classification are both methods of prediction through finding patterns in large sets of data. Regression predicts a numeric, continuous dependent variable. Conversely, classification involves the prediction of categorical outcomes. There are different, discrete classes of which algorithms attempt to assign observations through various methods.\n",
    "\n",
    "A confusion table helps us understand the precision of a model's performance. It is typically a square matrix modeling predicted performance by actual performance. In the testing phase of an algorithm, the model is run through a separate set of data (distinct from the training data) where the outcomes are already known. The model outputs its prediction, or classification, and the confusion table depicts its performance. Each class composes a row and column in the confusion matrix, with the result being an nxn matrix where every possible combination of actual and predicted values is shown. Counts of instances where the various combinations occur are tallied. An ideal model results in a diagonal matrix with true values matching predicted values. In practice, though, there are many false values.\n",
    "\n",
    "The SSE quantifies performance of a predictive model. This measure quantifies the precision of a model. A sum square of errors, the residual (e) is calculated as the difference between the predicted and actual values. The difference is then squared to get positive values. Finally, the resultant value is summed across all observations. This results in one scalar value indicative of the model's predictive performance.\n",
    "\n",
    "Overfitting is a case where too many data points are involved in the prediction of an outcome. This can result when, in the k nearest neighbor method, more data points are used to predict an outcome than are not. This can result in a gross generalization of predictions across observations, lacking meaningful insight for researchers and future predictions. Conversely, underfitting involves the use of too few data points to predict an outcome. This results when, in the k nearest neighbor method, limited data points are used to predict an outcome than are not. These data points often do not produce a generalizable average of outcome values that can be used to precisely predict future cases. This can equally result in a lack of meaningful insight for researchers.\n",
    "\n",
    "The applicability of a model across data is not guaranteed. In splitting data into training and testing sets, researchers can gain more confidence in the value of a model. The training set, a significant portion of the data as as whole, would be used to build the model. A testing set would be set aside to then run through the trained model and measure performance. If there is a high SSE, this may show a lack of generalizability of the model. Contrarily, a low SSE in testing the model may hint towards practical use of the model.\n",
    "\n",
    "Classification, unlike regression, does not have a method to find middle ground with the k nearest neighbor method. As a result, the two options available in predicting the outcome of observations is to use the modal result (most common outcome class among the neighbors) or measure probability distributions of all classes among the neighbors and pick the most frequent. Using the modal result can be a way to generate highly precise models with the data given (low SSE) but may not translate as well with other data sets. The generalizability will be poor if other data sets do not match outcome frequency. Conversely, the probability distribution over class labels allows for a more pragmatic approach to predicting outcomes. This can be good if there is a large distribution of class instances among the neighbors, where there is no clear modal outcome. The downside of this approach, though, can be an over-emphasis on rare (outlier) instances by taking away from the probability of higher frequency outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194455fa",
   "metadata": {},
   "source": [
    "**Q1.** This question is a case study for $k$ nearest neighbor regression, using the `USA_cars_datasets.csv` data.\n",
    "\n",
    "The target variable `y` is `price` and the features are `year` and `mileage`.\n",
    "\n",
    "1. Load the `./data/USA_cars_datasets.csv`. Keep the following variables and drop the rest: `price`, `year`, `mileage`. Are there any `NA`'s to handle? Look at the head and dimensions of the data.\n",
    "2. Maxmin normalize `year` and `mileage`.\n",
    "3. Split the sample into ~80% for training and ~20% for evaluation.\n",
    "4. Use the $k$NN algorithm and the training data to predict `price` using `year` and `mileage` for the test set for $k=3,10,25,50,100,300$. For each value of $k$, compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase $k$?\n",
    "5. Determine the optimal $k$ for these data.\n",
    "6. Describe what happened in the plots of predicted versus actual prices as $k$ varied, taking your answer into part 6 into account. (Hint: Use the words \"underfitting\" and \"overfitting\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab831de1-d6f9-4fcd-b210-084b1a1f97fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2499, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "cars = pd.read_csv(\"/Users/vaibhavjha/knn/data/USA_cars_datasets.csv\")\n",
    "cars = cars.loc[:,('price', 'year', 'mileage')]\n",
    "cars.head(15) # smaller df\n",
    "\n",
    "cars['price'].isnull().sum()\n",
    "cars['year'].isnull().sum()\n",
    "cars['mileage'].isnull().sum() # There are no NA's to handle\n",
    "\n",
    "cars.head(29)\n",
    "cars.shape # 2499 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6fbad87-bd3a-46bf-bdc4-ea06735171a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max min normalize year and mileage\n",
    "\n",
    "def maxmin(z):\n",
    "    z = (z-min(z))-(max(z)-min(z))\n",
    "    return(z)\n",
    "\n",
    "normcars = cars[['year', 'mileage']].apply(maxmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac22b740-d731-4393-becd-deb8cfd59f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(normcars, cars['price'], test_size=.2, random_state=100) # successfully split dataset into 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe994a4-a5f1-4962-9e49-bc27c950fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "for k in [3,10,25,50,100,300]:\n",
    "    model = KNeighborsRegressor(n_neighbors=k).fit(X_train,Y_train) # fit the model on the training data with k neighbors\n",
    "    Y_hat = model.predict(X_test) # performed prediction of Y_hat with input of X_test using the model built\n",
    "    SSE = np.sum( (Y_test-Y_hat)**2 ) # calculated the SSE of predicted values of y (Y_hat) difference with true y values (Y_test)\n",
    "    plot, axes = plt.subplots()\n",
    "    plt.scatter(Y_test,Y_hat) # made a scatterplot of predicted y values and actual y values within test set\n",
    "    plt.title('k: '+str(k)+', SSE: '+str(SSE)) # titles of plot\n",
    "    axes.set_ylim(-1000, 62000)\n",
    "    axes.set_xlim(-1000, 62000)\n",
    "    plt.show()\n",
    "\n",
    "# It seems that as k increases, the predictions tend to bunch together at the modal points of actual y. There is less variation among the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9662262b-da2b-4886-9e69-c4a52e8fa23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABGbUlEQVR4nO3deXhU9d3//9eZyUz2lUAWCAFRFAURQ1VABFGpKC61KlZvEQu23FotpfSryO2tWK9S28oPW4VbK6goFdqK1t5SbbwFBFFZBBdwQVkSICEkZF9mJjPn90fIyJiwBM/MSYbn47rmkjlzzuR9chLnlc92DNM0TQEAAEQJh90FAAAAWIlwAwAAogrhBgAARBXCDQAAiCqEGwAAEFUINwAAIKoQbgAAQFQh3AAAgKhCuAEAAFGFcAPghP3lL3/RvHnz2n3NMAw99NBDEa2n1ejRozV69Ohj7tenTx+NHz/+hL7GQw89JMMwZBiGkpKSQl5bu3atpkyZooKCAsXGxsowDO3atavNe3z55ZeaMWOGCgoKlJaWpoyMDI0YMUJ///vf2+w7b9684NczDEPl5eUnVDdwMiDcADhhRws37733nqZMmRLZgmzw3nvvaeXKlSHb/u///k9vvfWWevfureHDhx/x2H//+996/fXX9cMf/lB/+9vftGTJEp122mm64YYb9PDDD4fse9NNN+m9997T5MmTw3IeQDSJsbsAANHpggsusLuEiGjvPB944AE9+OCDkqQ//OEPWrVqVbvH3nTTTbrrrrtkGEZw27hx41ReXq5HH31U9957r2JjYyVJ2dnZys7O1htvvGH9SQBRhpYb4CSydu1aXXLJJUpOTlZCQoKGDx+u119/PWSf5557ToZhqLCwULfffrsyMjKUmJioq666Sjt27AjuN3r0aL3++uvavXt3SHdJq293S7W+79tvv6077rhD3bp1U0pKiiZOnKj6+nqVlpbqxhtvVFpamnJycjRjxgz5fL6Q2mbPnq3zzz9fGRkZSklJ0bnnnquFCxfKyvv/zp8/XzExMcFwciIcjuP7X2tmZmbI96zVeeedp4aGBh08ePCEawBOZoQb4CSxevVqjRkzRtXV1Vq4cKFeeuklJScn66qrrtKyZcva7D958mQ5HI5g19P69es1evRoVVVVSWoJASNGjFB2drbee++94ONYpkyZotTUVC1dulT/9V//pb/85S+64447dOWVV2rw4MH6+9//rttuu02PPfaY/vSnP4Ucu2vXLv30pz/VX//6Vy1fvlzXXXed7r77bv3617/+zt8f0zQ1Y8YMTZs2Tc8884xmz54dfG3SpElHHDcTDitXrlT37t3Vo0ePiHw9INrQLQWcJO677z6lp6dr1apVwQGw48eP1znnnKMZM2boxhtvDGlFGDp0qBYuXBh8ftZZZ2nEiBF68sknNWvWLJ155plKS0tTbGxsh7qgxo8frz/84Q+SpMsuu0zvvfeeXnrpJc2dO1e/+MUvJEmXXnqp3nzzTS1ZskTTp08PHvvss88G/x0IBDR69GiZpqnHH39cDzzwQLutIMejsbFRt956q9566y3961//0iWXXBLyutPplNPpPOH374hnnnlGq1at0uOPPy6n0xn2rwdEo5O65eadd97RVVddpdzcXBmGoVdffbVDxzc1NWnSpEkaNGiQYmJidO2117bZp6SkRDfffLNOP/10ORwOTZs2zZLagY6or6/XBx98oOuvvz5kZo/T6dStt96qPXv26Isvvgg55pZbbgl5Pnz4cOXn57cZPNtR356dNGDAAEnSlVde2Wb77t27Q7a9/fbbuvTSS5Wamiqn0ymXy6X//u//VkVFhcrKyk6onoqKCo0ZM0br168Pdtt928KFC9Xc3Kz8/PwT+hrH61//+pfuuusuXX/99br77rvD+rWAaHZSh5v6+noNHjxYTzzxxAkd7/f7FR8fr3vuuUeXXnppu/t4PB51795ds2bN0uDBg79LucAJq6yslGmaysnJafNabm6upJYP+cNlZ2e32Tc7O7vNfh2VkZER8tztdh9xe1NTU/D5+vXrNXbsWEnSn//8Z7377rvasGGDZs2aJaml9eVEfPnll/rggw80btw4DRw48ITewwpvvvmmrrvuOl122WVasmRJRFqJgGh1UndLjRs3TuPGjTvi616vV//1X/+lJUuWqKqqSgMHDtSjjz4aXD8jMTFRCxYskCS9++67wbEIh+vTp48ef/xxSdKiRYssPwfgeKSnp8vhcKikpKTNa/v27ZPUMrj1cKWlpW32LS0t1amnnhqeIo9h6dKlcrlc+t///V/FxcUFt3e0xfXbhg0bphtuuCE4xXrBggXHPSDYKm+++aauvfZajRo1Si+//HIw8AE4MSd1y82x3H777Xr33Xe1dOlSffzxx7rhhht0+eWXa/v27XaXBnRIYmKizj//fC1fvjykhSMQCOjFF19Ur1691L9//5BjlixZEvJ83bp12r17d8jieLGxsSfcYtJRhmEoJiYmZBxKY2OjXnjhhe/83rfddpuWLl2qZ599VhMnTpTf7//O73m8/v3vf+vaa6/VhRdeqFdffTU49RvAiTupW26O5uuvv9ZLL72kPXv2BJvtZ8yYoTfeeEPPPvusfvOb39hcIdAxc+bM0WWXXaaLL75YM2bMkNvt1vz58/Xpp5/qpZdeatMNsnHjRk2ZMkU33HCDiouLNWvWLPXs2VN33nlncJ9BgwZp+fLlWrBggQoKCuRwODR06NCw1H/llVdq7ty5uvnmm/WTn/xEFRUV+sMf/mBZGLj++uuVkJCg66+/Xo2NjXrppZeCLSiTJ0/W888/r6+//vq4xt0cOHBAq1evliR98sknklrG03Tv3l3du3fXqFGjJLVMzb/22muVnZ2t+++/X1u2bAl5nzPPPFMpKSmWnB9wMiHcHMGHH34o0zTb/DXr8XjUrVs3m6oCTtyoUaP09ttv68EHH9SkSZMUCAQ0ePBgvfbaa+3egmDhwoV64YUXdNNNN8nj8ejiiy/W448/HjI25uc//7m2bt2q+++/X9XV1TJN09I1Zw43ZswYLVq0SI8++qiuuuoq9ezZU3fccYd69Ohh2aq9V1xxhVasWKGrrrpK11xzjZYvX674+Hj5/X75/f7jPretW7fqhhtuCNnWGgpHjRoVXNTvrbfeUmNjo3bt2qUxY8a0eZ+VK1ce120kAIQyzHD9n6iLMQxDr7zySnDG07Jly3TLLbdo69atbaZjJiUltRlsOWnSJFVVVR21/3/06NE655xzjrhcPdAZPPfcc7r99tu1YcOGsLXCRIOHHnpIs2fPls/nk2EYYZ+2bZqm/H6/Hn74Yf3617/WgQMH2oyTAtCClpsjGDJkiPx+v8rKyjRy5Ei7ywHQSblcLiUmJqquri6sX+fxxx8PrgME4OhO6nBTV1enr776Kvh8586d2rJlizIyMtS/f3/dcsstmjhxoh577DENGTJE5eXlevvttzVo0CBdccUVkqRt27bJ6/Xq4MGDqq2tDfaZn3POOcH3bd1WV1enAwcOaMuWLXK73TrzzDMjdaoALPaTn/wk2J0XicX2br75Zl144YXB52lpaWH/mkBXdVJ3S61atUoXX3xxm+233XabnnvuOfl8Pj3yyCNavHix9u7dq27dumnYsGGaPXu2Bg0aJKllqve3FxqTFNI33956Ffn5+RFbyh0AgJOJreHmnXfe0e9//3tt2rRJJSUlIWNejmT16tWaPn26tm7dqtzcXP2///f/NHXq1MgUDAAAOj1b17np6ArBO3fu1BVXXKGRI0dq8+bNuv/++3XPPffo5ZdfDnOlAACgq+g03VLfnq3UnnvvvVevvfaaPvvss+C2qVOn6qOPPjquuxEDAIDo16UGFL/33nvBe8u0+v73v6+FCxfK5/PJ5XK1Ocbj8cjj8QSfBwIBHTx4UN26dePeLQAAdBGmaaq2tla5ubnHvEVKlwo3paWlysrKCtmWlZWl5uZmlZeXt3tTwDlz5mj27NmRKhEAAIRRcXGxevXqddR9ulS4kdrOPGrtVTtSK8zMmTM1ffr04PPq6mr17t1bxcXFLGsOAEAXUVNTo7y8PCUnJx9z3y4VbrKzs9vcqbisrEwxMTFHvCVCbGxsu/eeSUlJIdwAANDFHM+Qki51V/Bhw4apsLAwZNu///1vDR06tN3xNgAA4ORja7ipq6vTli1bgiv4tq4QXFRUJKmlS2nixInB/adOnardu3dr+vTp+uyzz7Ro0SItXLhQM2bMsKN8AADQCdnaLbVx48aQFYJbx8a0rhBcUlISDDqS1LdvX61YsUK/+MUv9OSTTyo3N1d//OMf9cMf/jDitQMAgM6p06xzEyk1NTVKTU1VdXU1Y24AAOgiOvL53aXG3AAAABwL4QYAAEQVwg0AAIgqhBsAABBVCDcAACCqEG4AAEBUIdwAAICoQrgBAABRpUvdOLMza/YHVFrTJNOU8jIS7C4HAICTFuHGIhX1Xl346Eo5HYa+/s0VdpcDAMBJi24pizgO3YLdHzip7mYBAECnQ7ixiNNhBP8dIOAAAGAbwo1FnMZh4ebkuhcpAACdCuHGIsZh30k/4QYAANsQbiwS0nITsLEQAABOcoQbixw+5oaWGwAA7EO4sYjjsJYbZkwBAGAfwo1FmC0FAEDnQLixyGHZhm4pAABsRLixiGEYwYBDyw0AAPYh3FiotWuKlhsAAOxDuLEQt2AAAMB+hBsLtbbcsM4NAAD2IdxYqLXlhtsvAABgH8KNhVoHFDPmBgAA+xBuLPRNtxThBgAAuxBuLMRsKQAA7Ee4sRCzpQAAsB/hxkLMlgIAwH6EGwsFW27olgIAwDaEGwsFx9zQLQUAgG0INxYKdkvRcgMAgG0INxYKrnNDyw0AALYh3FiIFYoBALAf4cZCzJYCAMB+hBsLMVsKAAD7EW4sxO0XAACwH+HGQg6mggMAYDvCjYWc3BUcAADbEW4sRLcUAAD2I9xYiAHFAADYj3BjIW6/AACA/Qg3FuL2CwAA2I9wYyHDYBE/AADsRrixELOlAACwH+HGQsyWAgDAfoQbCzFbCgAA+xFuLETLDQAA9iPcWIjbLwAAYD/CjYWcwW4pmwsBAOAkRrixEN1SAADYj3BjIQYUAwBgP8KNhZyHvpuMuQEAwD6EGwu1ttyYtNwAAGAbwo2FvpktZXMhAACcxAg3FnIy5gYAANsRbizEbCkAAOxHuLEQs6UAALAf4cZCrbOlaLkBAMA+hBsLcfsFAADsR7ixEAOKAQCwH+HGQgwoBgDAfraHm/nz56tv376Ki4tTQUGB1qxZc9T9n3zySQ0YMEDx8fE6/fTTtXjx4ghVemwMKAYAwH4xdn7xZcuWadq0aZo/f75GjBihp556SuPGjdO2bdvUu3fvNvsvWLBAM2fO1J///Gd973vf0/r163XHHXcoPT1dV111lQ1nEKo13NBwAwCAfWxtuZk7d64mT56sKVOmaMCAAZo3b57y8vK0YMGCdvd/4YUX9NOf/lQTJkzQKaecoptuukmTJ0/Wo48+GuHK28dsKQAA7GdbuPF6vdq0aZPGjh0bsn3s2LFat25du8d4PB7FxcWFbIuPj9f69evl8/nCVuvxYrYUAAD2sy3clJeXy+/3KysrK2R7VlaWSktL2z3m+9//vp555hlt2rRJpmlq48aNWrRokXw+n8rLy9s9xuPxqKamJuQRLsyWAgDAfrYPKDYOBYJWpmm22dbqgQce0Lhx43TBBRfI5XLpmmuu0aRJkyRJTqez3WPmzJmj1NTU4CMvL8/S+g/HbCkAAOxnW7jJzMyU0+ls00pTVlbWpjWnVXx8vBYtWqSGhgbt2rVLRUVF6tOnj5KTk5WZmdnuMTNnzlR1dXXwUVxcbPm5tPpmtlTYvgQAADgG28KN2+1WQUGBCgsLQ7YXFhZq+PDhRz3W5XKpV69ecjqdWrp0qcaPHy+Ho/1TiY2NVUpKSsgjXGi5AQDAfrZOBZ8+fbpuvfVWDR06VMOGDdPTTz+toqIiTZ06VVJLq8vevXuDa9l8+eWXWr9+vc4//3xVVlZq7ty5+vTTT/X888/beRpBDCgGAMB+toabCRMmqKKiQg8//LBKSko0cOBArVixQvn5+ZKkkpISFRUVBff3+/167LHH9MUXX8jlcuniiy/WunXr1KdPH5vOIBQDigEAsJ9hmifXJ3FNTY1SU1NVXV1teRfVsg1FuvflT3TJGT20cNL3LH1vAABOZh35/LZ9tlQ0MYIrFJ9UeREAgE6FcGMhJ7OlAACwHeHGQsyWAgDAfoQbCzFbCgAA+xFuLMRsKQAA7Ee4sRB3BQcAwH6EGws5aLkBAMB2hBsLMaAYAAD7EW4sFBxQTMsNAAC2IdxYKDigOGBzIQAAnMQINxZqHXNzkt3RAgCAToVwYyHHoe8m69wAAGAfwo2FWOcGAAD7EW4sxGwpAADsR7ixELOlAACwH+HGQq3dUgFmSwEAYBvCjYWc3DgTAADbEW4sxO0XAACwH+HGQgwoBgDAfoQbC7XeFZyWGwAA7EO4sZBh0HIDAIDdCDcWCs6WItsAAGAbwo2FmC0FAID9CDcWYhE/AADsR7ixkJMxNwAA2I5wYyEHs6UAALAd4cZCrS03pimZBBwAAGxBuLFQ64BiiUHFAADYhXBjIcfh4YaWGwAAbEG4sVBrt5TEncEBALAL4cZCDoOWGwAA7Ea4sZDjsO9mgHADAIAtCDcWCu2WItwAAGAHwo2FmC0FAID9CDcWMgxDrY03jLkBAMAehBuLfXMLBpsLAQDgJEW4sRg3zwQAwF6EG4tx80wAAOxFuLFY66BiBhQDAGAPwo3FHAwoBgDAVoQbi7WOuaFbCgAAexBuLBYcc0O2AQDAFoQbizkYcwMAgK0INxb7puWGcAMAgB0INxZjthQAAPYi3Fis9c7gzJYCAMAehBuLsYgfAAD2ItxYjAHFAADYi3BjsdaWG7qlAACwB+HGYk4HdwUHAMBOhBuLGbTcAABgK8KNxZyHvqOscwMAgD0INxZjthQAAPYi3FiM2VIAANiLcGMxbr8AAIC9CDcW+6blxuZCAAA4SRFuLMY6NwAA2ItwY7Fv1rkh3AAAYAfCjcUYUAwAgL0INxZztmQbuqUAALAJ4cZidEsBAGAvwo3FjOBUcJsLAQDgJEW4sRizpQAAsJft4Wb+/Pnq27ev4uLiVFBQoDVr1hx1/yVLlmjw4MFKSEhQTk6Obr/9dlVUVESo2mOjWwoAAHvZGm6WLVumadOmadasWdq8ebNGjhypcePGqaioqN39165dq4kTJ2ry5MnaunWr/va3v2nDhg2aMmVKhCs/MmZLAQBgL1vDzdy5czV58mRNmTJFAwYM0Lx585SXl6cFCxa0u//777+vPn366J577lHfvn114YUX6qc//ak2btwY4cqPrHW2FLdfAADAHraFG6/Xq02bNmns2LEh28eOHat169a1e8zw4cO1Z88erVixQqZpav/+/fr73/+uK6+88ohfx+PxqKamJuQRTrTcAABgL9vCTXl5ufx+v7KyskK2Z2VlqbS0tN1jhg8friVLlmjChAlyu93Kzs5WWlqa/vSnPx3x68yZM0epqanBR15enqXn8W0MKAYAwF62DyhunTrdyjTNNttabdu2Tffcc4/++7//W5s2bdIbb7yhnTt3aurUqUd8/5kzZ6q6ujr4KC4utrT+b2NAMQAA9oqx6wtnZmbK6XS2aaUpKytr05rTas6cORoxYoR+9atfSZLOPvtsJSYmauTIkXrkkUeUk5PT5pjY2FjFxsZafwJHwF3BAQCwl20tN263WwUFBSosLAzZXlhYqOHDh7d7TENDgxyO0JKdTqeklhafzoBuKQAA7GVrt9T06dP1zDPPaNGiRfrss8/0i1/8QkVFRcFuppkzZ2rixInB/a+66iotX75cCxYs0I4dO/Tuu+/qnnvu0Xnnnafc3Fy7TiPEoYabThO2AAA42djWLSVJEyZMUEVFhR5++GGVlJRo4MCBWrFihfLz8yVJJSUlIWveTJo0SbW1tXriiSf0y1/+UmlpaRozZoweffRRu06hDWZLAQBgL8M8yZoYampqlJqaqurqaqWkpFj+/o/87zY9s3anfjrqFM0cN8Dy9wcA4GTUkc9v22dLRRtmSwEAYC/CjcWYLQUAgL0INxZrnS3F7RcAALAH4cZiDCgGAMBehBuLsc4NAAD2ItxYzHnoO8qAYgAA7EG4sRjdUgAA2ItwYzFHcECxzYUAAHCSItxYjNlSAADYi3BjMbqlAACwF+HGYs5DN85kthQAAPYg3FiM2y8AAGAvwo3F6JYCAMBehBuLMaAYAAB7EW4sRssNAAD2ItxY7JvbL9hcCAAAJynCjcUYUAwAgL0INxY71HDDmBsAAGxCuLGYkzE3AADYinBjMWZLAQBgrw6Fm/Xr18vv9wefm9/6APd4PPrrX/9qTWVdFLOlAACwV4fCzbBhw1RRURF8npqaqh07dgSfV1VV6Uc/+pF11XVBzJYCAMBeHQo3326p+fbzI207mTBbCgAAe1k+5sZonS50kqJbCgAAezGg2GIMKAYAwF4xHT1g27ZtKi0tldTSBfX555+rrq5OklReXm5tdV2Q41BcpOUGAAB7dDjcXHLJJSHjasaPHy+ppTvKNM2TvlvqmwHFhBsAAOzQoXCzc+fOcNURNVrH3JBtAACwR4fCTX5+frjqiBoOgwHFAADYqUMDig8ePKg9e/aEbNu6datuv/123XjjjfrLX/5iaXFdEbdfAADAXh0KN3fddZfmzp0bfF5WVqaRI0dqw4YN8ng8mjRpkl544QXLi+xKmC0FAIC9OhRu3n//fV199dXB54sXL1ZGRoa2bNmif/zjH/rNb36jJ5980vIiuxJmSwEAYK8OhZvS0lL17ds3+Pztt9/WD37wA8XEtAzdufrqq7V9+3ZrK+xigisU03IDAIAtOhRuUlJSVFVVFXy+fv16XXDBBcHnhmHI4/FYVlxX5GRAMQAAtupQuDnvvPP0xz/+UYFAQH//+99VW1urMWPGBF//8ssvlZeXZ3mRXQm3XwAAwF4dmgr+8MMP67LLLtOLL76o5uZm3X///UpPTw++vnTpUo0aNcryIruSbwYU21wIAAAnqQ6FmyFDhuizzz7TunXrlJ2drfPPPz/k9ZtuuklnnnmmpQV2NUwFBwDAXh3qlvrggw+0ceNGXXPNNcFgs3jxYvXt21c9evTQP/7xD+Xm5oal0K6i9e4T3H4BAAB7dCjcPPTQQ/r444+Dzz/55BNNnjxZl156qe677z7985//1Jw5cywvsitxBm+/QLgBAMAOHQo3W7Zs0SWXXBJ8vnTpUp1//vn685//rOnTp+uPf/yj/vrXv1peZFfCbCkAAOzVoXBTWVmprKys4PPVq1fr8ssvDz7/3ve+p+LiYuuq64Icjm8GFNN6AwBA5HUo3GRlZQXvDO71evXhhx9q2LBhwddra2vlcrmsrbCLaW25kZgxBQCAHToUbi6//HLdd999WrNmjWbOnKmEhASNHDky+PrHH3+sfv36WV5kV9LaciPRNQUAgB06NBX8kUce0XXXXadRo0YpKSlJzz//vNxud/D1RYsWaezYsZYX2ZU4HYe33BBuAACItA6Fm+7du2vNmjWqrq5WUlKSnE5nyOt/+9vflJSUZGmBXc3h3VK03AAAEHkdCjetUlNT292ekZHxnYqJBo7DOvpY6wYAgMjr0JgbHFvIgGJabgAAiDjCjcUcdEsBAGArwo3FHA6mggMAYCfCTRg4gwv5kW4AAIg0wk0YcAsGAADsQ7gJg9YZU4QbAAAij3ATBq0tN3RLAQAQeYSbMGgdVEzLDQAAkUe4CQMGFAMAYB/CTRh8M6DY5kIAADgJEW7CgG4pAADsQ7gJg9Z1/OiWAgAg8gg3YcBsKQAA7EO4CQO6pQAAsA/hJgyYLQUAgH0IN2HAbCkAAOxje7iZP3+++vbtq7i4OBUUFGjNmjVH3HfSpEkyDKPN46yzzopgxcdGtxQAAPaxNdwsW7ZM06ZN06xZs7R582aNHDlS48aNU1FRUbv7P/744yopKQk+iouLlZGRoRtuuCHClR9dnKvl29rU7Le5EgAATj62hpu5c+dq8uTJmjJligYMGKB58+YpLy9PCxYsaHf/1NRUZWdnBx8bN25UZWWlbr/99ghXfnQJrhhJUqOXcAMAQKTZFm68Xq82bdqksWPHhmwfO3as1q1bd1zvsXDhQl166aXKz88/4j4ej0c1NTUhj3BLiHVKkuo9zWH/WgAAIJRt4aa8vFx+v19ZWVkh27OyslRaWnrM40tKSvSvf/1LU6ZMOep+c+bMUWpqavCRl5f3neo+HonulpabBlpuAACIONsHFBuHZha1Mk2zzbb2PPfcc0pLS9O111571P1mzpyp6urq4KO4uPi7lHtc4t0tLTeEGwAAIi/Gri+cmZkpp9PZppWmrKysTWvOt5mmqUWLFunWW2+V2+0+6r6xsbGKjY39zvV2RGIw3NAtBQBApNnWcuN2u1VQUKDCwsKQ7YWFhRo+fPhRj129erW++uorTZ48OZwlnrCE2JbMWO+h5QYAgEizreVGkqZPn65bb71VQ4cO1bBhw/T000+rqKhIU6dOldTSpbR3714tXrw45LiFCxfq/PPP18CBA+0o+5gSXC0tN40+Wm4AAIg0W8PNhAkTVFFRoYcfflglJSUaOHCgVqxYEZz9VFJS0mbNm+rqar388st6/PHH7Sj5uNByAwCAfWwNN5J055136s4772z3teeee67NttTUVDU0NIS5qu+GMTcAANjH9tlS0ah1thQtNwAARB7hJgyC69z4CDcAAEQa4SYMWlcobmCFYgAAIo5wEwYJrFAMAIBtCDdhwIBiAADsQ7gJg+BUcFpuAACIOMJNGLQu4udtDqjZH7C5GgAATi6EmzBoHVAsMWMKAIBII9yEgdvpUIyj5c7mDax1AwBARBFuwsAwjG8W8mNQMQAAEUW4CZPWhfwaGVQMAEBEEW7CpHXcTT0L+QEAEFGEmzBJCK51Q8sNAACRRLgJk9ZVihlzAwBAZBFuwiSRlhsAAGxBuAmT4P2lGHMDAEBEEW7CJCE4FZyWGwAAIolwEyaJsUwFBwDADoSbMGERPwAA7EG4CZPggGJuvwAAQEQRbsIkOKCYG2cCABBRhJswCS7ix2wpAAAiinATJgmxLOIHAIAdCDdh0jrmhtlSAABEFuEmTOJZ5wYAAFsQbsIkkRWKAQCwBeEmTBJjabkBAMAOhJswiXezQjEAAHYg3IRJ64Birz8gb3PA5moAADh5EG7CpHURP4nWGwAAIolwEybuGIdiHIYkqcHHoGIAACKFcBNGrasU13N/KQAAIoZwE0aJh1YpbmCVYgAAIoZwE0atC/k1MOYGAICIIdyEUXAhP1puAACIGMJNGDHmBgCAyCPchFECN88EACDiCDdhlHBoQHE93VIAAEQM4SaMEhlQDABAxBFuwigp1iVJqm702VwJAAAnD8JNGPXJTJAk7ThQb3MlAACcPAg3YXRq9yRJ0ldltTZXAgDAyYNwE0anZrWEm6KDDWryMe4GAIBIINyEUfekWKXGuxQw6ZoCACBSCDdhZBiGTuvR0nqzna4pAAAignATZqcd6pr6uqzO5koAADg5EG7CrF/31pYbwg0AAJFAuAmz07KSJRFuAACIFMJNmLWOudlVXi9vc8DmagAAiH6EmzDLSY1Totup5oCp3RXMmAIAINwIN2FmGIZOpWsKAICIIdxEwDcrFRNuAAAIN8JNBLROB6flBgCA8CPcRAAtNwAARA7hJgL6ZCZKkooq6mWaps3VAAAQ3Qg3EZCXES/DkOq9fpXXee0uBwCAqEa4iYDYGKdyUuIkSUUHmQ4OAEA4EW4iJL9bS9fU7ooGmysBACC6EW4iJL9bgiRpF+EGAICwItxESGvLTRGrFAMAEFaEmwih5QYAgMgg3ERIa7gpOki4AQAgnGwPN/Pnz1ffvn0VFxengoICrVmz5qj7ezwezZo1S/n5+YqNjVW/fv20aNGiCFV74lq7pQ7We1XT5LO5GgAAoleMnV982bJlmjZtmubPn68RI0boqaee0rhx47Rt2zb17t273WNuvPFG7d+/XwsXLtSpp56qsrIyNTc3R7jyjkuKjVFmklvldV4VVTRoYM9Uu0sCACAq2Rpu5s6dq8mTJ2vKlCmSpHnz5unNN9/UggULNGfOnDb7v/HGG1q9erV27NihjIwMSVKfPn0iWfJ30jsjQeV1Xu2qqCfcAAAQJrZ1S3m9Xm3atEljx44N2T527FitW7eu3WNee+01DR06VL/73e/Us2dP9e/fXzNmzFBjY+MRv47H41FNTU3Iwy6sdQMAQPjZ1nJTXl4uv9+vrKyskO1ZWVkqLS1t95gdO3Zo7dq1iouL0yuvvKLy8nLdeeedOnjw4BHH3cyZM0ezZ8+2vP4T0TqoeDfTwQEACBvbBxQbhhHy3DTNNttaBQIBGYahJUuW6LzzztMVV1yhuXPn6rnnnjti683MmTNVXV0dfBQXF1t+Dsfrm3BDyw0AAOFiW8tNZmamnE5nm1aasrKyNq05rXJyctSzZ0+lpn4zXmXAgAEyTVN79uzRaaed1uaY2NhYxcbGWlv8CaJbCgCA8LOt5cbtdqugoECFhYUh2wsLCzV8+PB2jxkxYoT27dunurq64LYvv/xSDodDvXr1Cmu9VsjPaGm5Ka1pUpPPb3M1AABEJ1u7paZPn65nnnlGixYt0meffaZf/OIXKioq0tSpUyW1dClNnDgxuP/NN9+sbt266fbbb9e2bdv0zjvv6Fe/+pV+/OMfKz4+3q7TOG4ZiW7Fu5ySpJLqJpurAQAgOtk6FXzChAmqqKjQww8/rJKSEg0cOFArVqxQfn6+JKmkpERFRUXB/ZOSklRYWKi7775bQ4cOVbdu3XTjjTfqkUcesesUOsQwDOWmxenrA/XaV9WovpmJdpcEAEDUMUzTNO0uIpJqamqUmpqq6upqpaSkRPzr37rwA63ZXq7fX3+2bhiaF/GvDwBAV9SRz2/bZ0udbHJS4yRJ+6rolgIAIBwINxGWm9YyNqik+sgLDwIAgBNHuImw3NSWcLO3inADAEA4EG4i7JuWG7qlAAAIB8JNhOWktY65adRJNpYbAICIINxEWGu3VIPXr5rGZpurAQAg+hBuIize7VR6gksS424AAAgHwo0NmDEFAED4EG5skHOoa2ofg4oBALAc4cYGPQ8bVAwAAKxFuLFBTmu3FOEGAADLEW5s0DrmhlswAABgPcKNDXJb7y/FgGIAACxHuLFBa8tNaXWT/AEW8gMAwEqEGxv0SI6Vw5CaA6bK6zx2lwMAQFQh3NggxulQdkpL1xQL+QEAYC3CjU1au6b2VhJuAACwEuHGJnkZCZKkooMNNlcCAEB0IdzYpHdruKkg3AAAYCXCjU3yu9FyAwBAOBBubEK4AQAgPAg3Nmkdc7OvulGeZr/N1QAAED0INzbpnhSrBLdTpsmMKQAArES4sYlhGMFBxbvpmgIAwDKEGxvlMWMKAADLEW5slM9aNwAAWI5wY6PWGVO7abkBAMAyhBsb9e6WKEkqOlhvcyUAAEQPwo2Neh/WLWWaps3VAAAQHQg3NuqZFi+HITX5AjpQ67G7HAAAogLhxkbuGEfw7uBMBwcAwBqEG5sFb8PAoGIAACxBuLFZ67ibneUMKgYAwAqEG5ud3StNkrTkg90qr2PcDQAA3xXhxmbXF/TSGdnJqmzwafY/t9ldDgAAXR7hxmYup0O/v36wHIb0z4/2qXDbfrtLAgCgSyPcdAKDeqXqjotOkSQ9+sbnNlfz3VQ1eLVw7U698N4urdl+QMUHG+QPsIYPACByYuwuAC3uuvhULVyzU1+V1Wl3Rb3yD61e3FWYpqkVn5Tqwdc+VXmdN+Q1t9OhM3NT9IvL+mtU/+42VfiNdV+Va9nGYmWnxunsnmm64JQMdUuKlSTtqWzQxl2V2l5Wq5KqJg3smapRp3fXKZmJMgzD5soBAMeDcNNJpMS5NLRPut7fcVCrvjig24Z3nXDjafZr5vJPtPzDvZKkU7on6pTMRO0sr1fRwQZ5/QFtKa7SbYvW6+LTu+tnY07Vub3TIxIWig82aMrzG+WOceiui09VSXWjfv2/23R4Y5JhSAW90+VpDuiTvdUhxy/fvFf6X+n8vhmac90gndI9SU0+v8pqPOqREqs4lzPs5wAA6BjDPMnW/a+pqVFqaqqqq6uVkpJidzkhnlr9teb863ONPr27nrv9PLvLOS7ldR799IVN2rS7Uk6HobtG99NdY05VbEzLh74/YGpPZYNeeG+3nn9vl3z+lh+3M7KT9eMRfXXtkJ5yx7TtHTVNU/trPMpIdLf7+uH21zTpH1v2qs7j11Vn5+i0rGRJLa0wE556X3urGtscc+WgHKUmuPTh7kp9Xlob3O4wpHPy0jQgJ0Xdk2O1cVel1u88KK8/IHeMQ0Py0rSluEqe5oAkKSc1Ttec01M/uegUpSe4tK+6Sd7mgPp0S6ClBwAs1JHPb8JNJ/Ll/lqN/f/eUWyMQx89OLbTtwo0eJt19RPv6quyOqXExWj+LQW68LTMI+6/s7xeC1Z9pdc+2qcm3zfhYFi/bjpY71WTz69uibGKjXHog50HtbeqUTmpcfr1NQN16ZlZbd6vos6jWa98qn9vKw1piRnYM0XdEmP1RWmtSmua1DczUZcPzNbidbvU4PNr5rgzdMfIU4LhY19Vo1Z9cUAxDkNjBvRQ5qEuqlZ7Kht0/yuf6p0vDwS3uZxGMKhJUqLbqcTYGJUduo1GZlKsRpzaTT+56BSdlZva8W8uACAE4eYoOnO4MU1TFz66UnurGvXs7d/Txaf36PB71HmaNfm5Ddpb1aieafEa0jtd0y497biCUm2TT57mgJp8fm3YdVCrvzigeHeM7hzdT3mHFhs83L1//1jLNhYrKyVWf7njAvXrnnRcNVY3+LR0Q5EWrt0ZDAPHcsEpGTonL11nZCdrSO80NfkCmvz8Bu2pbGmVGZqfrrQEt1Z+URYygDm/W4KW/WSYslPjVN3oU02jr91zORbTNPX252XaV92kC/pm6NQeSaps8GnjroP649vb9eneGklSjMOQw2HIe6hlxzCk64b00oCcZNV7/EpPdOmM7BSdnp2s1HhXh+sAgJMV4eYoOnO4kaT7X/lEf/mgSLcNy9fsawZ2+Phf/e0j/W3TnpBtVw3O1R9vOkeGYeizkhp9sKNCW4qrVFHvVYzDUL3Xr6/L6lRR7233PeNcDv30on4af3aOTu2RJMMw9M+P9unulzbLMKSX7rhAF5zSrcO1Nvn8ev3jEpXWNKl7Uqzi3E5V1HlU29SsQb1SNbhXmp5+Z4f+vGbHEWdc5XdL0P/8R4EG5LRcy7KaJq3fdVBNvoAMSZcM6KG0BHeHa+sI0zS1cXelJGlQz1QZhrSlqEovflCkf36074jH9UyL16k9kpTgdsod49CIfpm6ZkhusEsPAPANws1RdPZwU7htv+5YvFG9MxK0+lejOzRu41+flOg/l3wow5B+98Oz1dQc0OzXtqo5YGryhX1VfLBB/z6OdXRiHIZO7ZGki8/ooc1FlXp/x8HgaylxMTJNqdbTLEm6Z8ypmj729I6faAd8faBO674q15f76/Tpvmp9urdaPr+p8/pm6Kn/KFB6YnjDy3exuahSL7y3W80BU4mxTpXVePR5aW2744Cklu6s6wt66fxTMnRu7/Swte5UN/r0WUmNKuq8Soh1Kik2JvjITYuX08F4IXw3rX+Q8LMEqxBujqKzh5t6T7OG/LpQ3uaAnrz5XF15ds5xHVdW06Sx895RVYNP/zm6n+69/AxJ0kvrizRz+SfB/ZwOQyNPy9S5vdPVKz1e/oApl9Ohft2TdEr3RCW4nSGByjRN/fPjEr30QZE2F1cGx8pI0qUDsvQ//3GuYpyRXS6pyefX/pom9c7ouoN2qxt8+ry0Rrsq6uVtDqii3qtlG4pVUt0U3McwpDOyU3R2z1R5mv2qbPDptB5JGj84V4N7pQbHGR3pw8M0TdV7/fI1B5SW0BKS3vi0VI//3/aQQdTflhwbo4I+6erXPUmJbqdSE9w6pXui+nRLVLM/oAavXzlpceqRHHfUcwwETO0or1dzIKD+PZLlcBiq9zTroz1VOljvVU1jszKT3CrITw9OxYe9DtR69P6OCh2s98rnD6je41dlg1eNXr96pMQqNy1euWnx6pnWcu3L67yqqPOqot6jqgafHIbkD0ibi1sG4vv8AfXOSFBmUqyqGnyqafIpNd6l7smxSnA7FeN0yNccUFWDT1WNXlU2tHQdSy0/107DkGFImcmxGtEvU+efkqH0BLdcTodKqhtVVNGgeq9fUsvvixH8b8txknT4J5zDYSg5NkbxbqcavM2q8/iVFu9SXkaCUuNdavYH5PUH1Ow31RwwFeMwFOM0lJ7gVlZKnHz+gL4ordWuinodrPequtGn5LiW8+mZFqf8bi2/J/FuWl/DgXBzFJ093EjSY//+Qn96+yulJbj072kXqUfK0T9EJOmh17bquXW7NLBnipb/54iQGUZzVnymp97ZoVH9u+uB8QN0ao/kE6rL2xzQV2V1inU5lJ7gVnqCq8uGi87I5w/ojU9LtfrLA9q466B2HeVO8Q5DCphSvMup687tqQnfy9Mne6v1xqel2lvZqOpGn6obfWo+lIDSElxKjXdp92Hv2Ss9Xrmp8ar3Nqve0/I/+pomX3C80LFkp8RpUK9Und0zVf2zkxXvcspvmvp0T7U2FVVqc1GVqg99UKUluJTfLVHb9lWHDMRulZnklsMw5I5xqG9movpnJatHcqxS4l3qn5Wsc3unWfqzZpqm9lU36ZM9Vdq6r0ap8S6N6t892O3aqtkfkNNhyDAMNXr9+r/P9+u9ryv0WUmNiitbxrWd0j1RlfVefXWgToGAlJsWp17pCTq1R5J6JMdqS3GV1u88KIdhKDs1Tn26JWhI73Sd2iNJDd6W8FDV4FVVg08xTofSE1xKjI2RwzAUME1VN/hU2dDywV/d6FVsjFO90uOVlRKnpLgYxTgM7a5oUNHBBqXGu9Sve5KyU+MUG+NQWoJL2SlxMgxD/oCpHQfqVFLdpMoGr4oPNuirsjrtq2qS1x9QbZNPXx/gBr5WyEqJVXZKnCobfKqo8ygtwa1e6fHKTI4NTj5IdMcozuVQo8+veo9f8W6nMhLcSomPUZzLqQR3jBLcTsW5WlpWE2OdcjsdkiE5DONQkDPkMFr+m+B2ynXoD03TNGWaLWEumhBujqIrhBtvc0A/mP+utu6r0Xl9M9Sve6K27qvR3WNO02XtzBoqr/NoxG/flqc5oCVTzteIU9vOWKpu8Ck1gQGsXUlZTZM27q7U5yU1So5zKSkuRuu+rtBb2/ar0ec/ofdMcDs1ZeQpun14n3a78/wBU5+V1GjjroMqqWlSo9ev8jqPviqrU/HBRsW6HIqLcWp/bZOO5/8ccS6HnIYR/Otaahlr1Cs9XslxMdpd0aDtZXXHfJ+8jHhdckaWUuJilBQXo8G90jSkd7oqG7z6YOdBFR9sUNWhAFDV4FODt1lnZKfovL7pavD6tXVfjQ7UemRKqmn06dO91e2OMUtPcKlnerxS4lqC4N6qRiXHxSgvPUG7KurV4D2x77ud0hJcys9I0NcH6lV3qDv5aM7MSVHfzES5nIbi3U6lJ7gV53Jqf02T9lU1qqS6SXurGuUwDHVLdKtbklvdEmODrYMB01T/rGQN75eplPgY7SxvaeXISHQrOc6l6kafDtR61OTzy+cPBANdWrw7GMINQwoEJL9pBkPZ2q/K9cneajV4/PI0+9UjJU75GQlKiXfJNCVTZvBn0jRNmWppyZEUDKzNgYDqmprV4PW3BIxYpw7We1V8sFH1nma5nA7FOI2W/zoMNQdM+fwBHaz3qqzWI4ch9euepFN7JCkzKVYpcTGqaWrWgTqP9lQ2andFvaoafGG4iscnzuWQaSq4VEVSbIxS4mLUu1uC+mYm6ZTMRPXNTJTTYaistkkNXn/LH6qJbmUkuJWe6FJm0jfrd307JJmmGZx00vr/oHhXSwCLjXGE/Y9dws1RdIVwI0nb99fqyj+tDfkr+pTMRP3fL0fJMAy1XjbDMPT7Nz/Xkyu/1uC8NL1653BaU6Jck8+v6kaf3E6HPi+t1cK1O/T252U6MzdF48/O1Tl5aUo79GHR+kHx9YE67alsVEF+epup7iei3tOsrftq9PGeKn2yt1q7KhrkbQ4oEDDVPztZBb3TdG5+enCg98d7qrWnskFD8tLVu1vobLWqBq/2VTXJlKlGr19fldVpe1ldS2tFfUt4aS9UuJ0Oef3H18rUnhiHodOzkzUwN1X7qhv1wc6Dx2y1ysuI1+VnZWtQrzT1zkjQ3spG7SyvU1qCW6f2SAp2l+yuaNDXZXXaU9WoM3NSNLxfN8W5nCqpbtQXpXXaXFypoooGpcS7lJbgUnqCW2nxLvkCpqoavKr3NAe7HYOvH7qmDb5m7a1sVFmtR/WeZnmbA8rLSFB+twRVNfj0VVmdDtZ75Wn2q6rhm9Y7qSXc9s5IUHqCWzmpcerXI0m9MxIU52oZ1D4wN4UuwiPwB0yZpnnMbviqBq92VTSorKZJGYluZSS6Vdng057KBlXWe1Xv9ave09Ja2uQLKN7tVGKsUw1evw7We1Xb1KxGr18NPr+avH7Ve1vCWJ2nWc3+gEzpuP6w+C5S4mLkcjpU0+STz2/KYUgxjqP/vhlGS9BpDTtZKbFafucIS+si3BxFVwk3kvTq5r1a/N4uDc5L0183FKve69fSn1yg8/pkaNJzG7S5qFK3nJ+vJe/vVq2nWU/fWqCxZ2XbXTZsYJpm1IbaBm+zCrft16d7q9XkC6ii3qMPdhxURb1XDkM6MzdFZ2SnKCOxJcylJ7gV4zT0UXGVPiyqUlKsU2flpqpXerwchqE4l1MDcpI1ICclZImERq9fO8rrVFrdpOpDSwbkZySoqtGn3RUN6pEcq7N7pXap77On2a/t++u0u6JB/Xok6rQeyQzwjRKtrSqmWlqkGjx+1TY1yzAU/Lmu9zSrot6r3RX12ln+zcM0W7rOEtwxqmr06mC9T5X1Xh2s9x73HwwxjtbWsPYjRE5qnN6beYkl59qKcHMUXSncHG7m8o/10vpiXXtOrkae1l2//NtHIa/3z0rSGz+/KOr6WIH2mKap3RUNykhyKyWO7lbACqZptnSz1TapOWAqOc6leJdTzf6AmgOm3DEOxbmciotxBFuwfP5vuqmavAE1Hvq3aZoa0jvd0vo68vnNvaW6iJu+11svrS/Wik9L9e7XFZKk64b01Jdltdq2r0a/+v4ZBBucNAzDUJ/MrnP/NaArMAxDqfGuDi1B4XI65HI6lNzJ/sgg3HQRZ/dK1YCcFH1W0jIwsndGgub8cJDcToc8zYFOf6sGAAAiJbILlOCEGYahH52XF3x+/xUDFBvTsiYNwQYAgG/QctOF/GBITy3/cK9O6Z6o75/Vdko4AAAg3HQpyXEuvXqXtVPrAACINnRLAQCAqEK4AQAAUYVwAwAAogrhBgAARBXCDQAAiCqEGwAAEFUINwAAIKrYHm7mz5+vvn37Ki4uTgUFBVqzZs0R9121apUMw2jz+PzzzyNYMQAA6MxsDTfLli3TtGnTNGvWLG3evFkjR47UuHHjVFRUdNTjvvjiC5WUlAQfp512WoQqBgAAnZ2t4Wbu3LmaPHmypkyZogEDBmjevHnKy8vTggULjnpcjx49lJ2dHXw4ndxbCQAAtLAt3Hi9Xm3atEljx44N2T527FitW7fuqMcOGTJEOTk5uuSSS7Ry5cpwlgkAALoY2+4tVV5eLr/fr6ys0BtAZmVlqbS0tN1jcnJy9PTTT6ugoEAej0cvvPCCLrnkEq1atUoXXXRRu8d4PB55PJ7g85qaGutOAgAAdDq23zjTMIyQ56ZpttnW6vTTT9fpp58efD5s2DAVFxfrD3/4wxHDzZw5czR79mzrCgYAAJ2abd1SmZmZcjqdbVppysrK2rTmHM0FF1yg7du3H/H1mTNnqrq6OvgoLi4+4ZoBAEDnZ1vLjdvtVkFBgQoLC/WDH/wguL2wsFDXXHPNcb/P5s2blZOTc8TXY2NjFRsbG3xumqYkuqcAAOhKWj+3Wz/Hj8bWbqnp06fr1ltv1dChQzVs2DA9/fTTKioq0tSpUyW1tLrs3btXixcvliTNmzdPffr00VlnnSWv16sXX3xRL7/8sl5++eXj/pq1tbWSpLy8POtPCAAAhFVtba1SU1OPuo+t4WbChAmqqKjQww8/rJKSEg0cOFArVqxQfn6+JKmkpCRkzRuv16sZM2Zo7969io+P11lnnaXXX39dV1xxxXF/zdzcXBUXFys5OfmIY3tOVE1NjfLy8lRcXKyUlBRL37uziPZzjPbzkzjHaBDt5ydF/zlG+/lJ1p+jaZqqra1Vbm7uMfc1zONp38FxqampUWpqqqqrq6P6hzWazzHaz0/iHKNBtJ+fFP3nGO3nJ9l7jrbffgEAAMBKhBsAABBVCDcWio2N1YMPPhgyOyvaRPs5Rvv5SZxjNIj285Oi/xyj/fwke8+RMTcAACCq0HIDAACiCuEGAABEFcINAACIKoQbAAAQVQg3Fpk/f7769u2ruLg4FRQUaM2aNXaXdMLmzJmj733ve0pOTlaPHj107bXX6osvvgjZZ9KkSTIMI+RxwQUX2FRxxzz00ENtas/Ozg6+bpqmHnroIeXm5io+Pl6jR4/W1q1bbay44/r06dPmHA3D0F133SWpa16/d955R1dddZVyc3NlGIZeffXVkNeP57p5PB7dfffdyszMVGJioq6++mrt2bMngmdxZEc7P5/Pp3vvvVeDBg1SYmKicnNzNXHiRO3bty/kPUaPHt3mut50000RPpMjO9Y1PJ6fy858DaVjn2N7v5eGYej3v/99cJ/OfB2P5/OhM/wuEm4ssGzZMk2bNk2zZs3S5s2bNXLkSI0bNy7k1hFdyerVq3XXXXfp/fffV2FhoZqbmzV27FjV19eH7Hf55ZerpKQk+FixYoVNFXfcWWedFVL7J598Enztd7/7nebOnasnnnhCGzZsUHZ2ti677LLgfcm6gg0bNoScX2FhoSTphhtuCO7T1a5ffX29Bg8erCeeeKLd14/nuk2bNk2vvPKKli5dqrVr16qurk7jx4+X3++P1Gkc0dHOr6GhQR9++KEeeOABffjhh1q+fLm+/PJLXX311W32veOOO0Ku61NPPRWJ8o/Lsa6hdOyfy858DaVjn+Ph51ZSUqJFixbJMAz98Ic/DNmvs17H4/l86BS/iya+s/POO8+cOnVqyLYzzjjDvO+++2yqyFplZWWmJHP16tXBbbfddpt5zTXX2FfUd/Dggw+agwcPbve1QCBgZmdnm7/97W+D25qamszU1FTzf/7nfyJUofV+/vOfm/369TMDgYBpml37+pmmaUoyX3nlleDz47luVVVVpsvlMpcuXRrcZ+/evabD4TDfeOONiNV+PL59fu1Zv369KcncvXt3cNuoUaPMn//85+EtziLtneOxfi670jU0zeO7jtdcc405ZsyYkG1d6Tp++/Ohs/wu0nLzHXm9Xm3atEljx44N2T527FitW7fOpqqsVV1dLUnKyMgI2b5q1Sr16NFD/fv31x133KGysjI7yjsh27dvV25urvr27aubbrpJO3bskCTt3LlTpaWlIdczNjZWo0aN6rLX0+v16sUXX9SPf/zjkJvFduXr923Hc902bdokn88Xsk9ubq4GDhzYJa9tdXW1DMNQWlpayPYlS5YoMzNTZ511lmbMmNGlWhylo/9cRts13L9/v15//XVNnjy5zWtd5Tp++/Ohs/wu2npX8GhQXl4uv9+vrKyskO1ZWVkqLS21qSrrmKap6dOn68ILL9TAgQOD28eNG6cbbrhB+fn52rlzpx544AGNGTNGmzZt6vQrbp5//vlavHix+vfvr/379+uRRx7R8OHDtXXr1uA1a+967t69245yv7NXX31VVVVVmjRpUnBbV75+7Tme61ZaWiq326309PQ2+3S139Wmpibdd999uvnmm0NuSHjLLbeob9++ys7O1qeffqqZM2fqo48+CnZLdnbH+rmMpmsoSc8//7ySk5N13XXXhWzvKtexvc+HzvK7SLixyOF/EUstF/3b27qin/3sZ/r444+1du3akO0TJkwI/nvgwIEaOnSo8vPz9frrr7f5Re1sxo0bF/z3oEGDNGzYMPXr10/PP/98cPBiNF3PhQsXaty4ccrNzQ1u68rX72hO5Lp1tWvr8/l00003KRAIaP78+SGv3XHHHcF/Dxw4UKeddpqGDh2qDz/8UOeee26kS+2wE/257GrXsNWiRYt0yy23KC4uLmR7V7mOR/p8kOz/XaRb6jvKzMyU0+lskzbLysraJNeu5u6779Zrr72mlStXqlevXkfdNycnR/n5+dq+fXuEqrNOYmKiBg0apO3btwdnTUXL9dy9e7feeustTZky5aj7deXrJ+m4rlt2dra8Xq8qKyuPuE9n5/P5dOONN2rnzp0qLCwMabVpz7nnniuXy9Vlr+u3fy6j4Rq2WrNmjb744otj/m5KnfM6HunzobP8LhJuviO3262CgoI2zYWFhYUaPny4TVV9N6Zp6mc/+5mWL1+ut99+W3379j3mMRUVFSouLlZOTk4EKrSWx+PRZ599ppycnGBT8OHX0+v1avXq1V3yej777LPq0aOHrrzyyqPu15Wvn6Tjum4FBQVyuVwh+5SUlOjTTz/tEte2Ndhs375db731lrp163bMY7Zu3Sqfz9dlr+u3fy67+jU83MKFC1VQUKDBgwcfc9/OdB2P9fnQaX4XLRmWfJJbunSp6XK5zIULF5rbtm0zp02bZiYmJpq7du2yu7QT8p//+Z9mamqquWrVKrOkpCT4aGhoME3TNGtra81f/vKX5rp168ydO3eaK1euNIcNG2b27NnTrKmpsbn6Y/vlL39prlq1ytyxY4f5/vvvm+PHjzeTk5OD1+u3v/2tmZqaai5fvtz85JNPzB/96EdmTk5Olzi3w/n9frN3797mvffeG7K9q16/2tpac/PmzebmzZtNSebcuXPNzZs3B2cLHc91mzp1qtmrVy/zrbfeMj/88ENzzJgx5uDBg83m5ma7TivoaOfn8/nMq6++2uzVq5e5ZcuWkN9Lj8djmqZpfvXVV+bs2bPNDRs2mDt37jRff/1184wzzjCHDBnSKc7PNI9+jsf7c9mZr6FpHvvn1DRNs7q62kxISDAXLFjQ5vjOfh2P9flgmp3jd5FwY5Enn3zSzM/PN91ut3nuueeGTJvuaiS1+3j22WdN0zTNhoYGc+zYsWb37t1Nl8tl9u7d27ztttvMoqIiews/ThMmTDBzcnJMl8tl5ubmmtddd525devW4OuBQMB88MEHzezsbDM2Nta86KKLzE8++cTGik/Mm2++aUoyv/jii5DtXfX6rVy5st2fy9tuu800zeO7bo2NjebPfvYzMyMjw4yPjzfHjx/fac77aOe3c+fOI/5erly50jRN0ywqKjIvuugiMyMjw3S73Wa/fv3Me+65x6yoqLD3xA5ztHM83p/LznwNTfPYP6emaZpPPfWUGR8fb1ZVVbU5vrNfx2N9Pphm5/hdNA4VCwAAEBUYcwMAAKIK4QYAAEQVwg0AAIgqhBsAABBVCDcAACCqEG4AAEBUIdwAAICoQrgBEBVGjx6tadOm2V0GgE6AcAMAAKIK4QYAAEQVwg2AqPTGG28oNTVVixcvtrsUABFGuAEQdZYuXaobb7xRixcv1sSJE+0uB0CEEW4ARJX58+dr6tSp+sc//qFrrrnG7nIA2CDG7gIAwCovv/yy9u/fr7Vr1+q8886zuxwANqHlBkDUOOecc9S9e3c9++yzMk3T7nIA2IRwAyBq9OvXTytXrtQ//vEP3X333XaXA8AmdEsBiCr9+/fXypUrNXr0aMXExGjevHl2lwQgwgg3AKLO6aefrrffflujR4+W0+nUY489ZndJACLIMOmYBgAAUYQxNwAAIKoQbgAAQFQh3AAAgKhCuAEAAFGFcAMAAKIK4QYAAEQVwg0AAIgqhBsAABBVCDcAACCqEG4AAEBUIdwAAICoQrgBAABR5f8Hfp5Tx5RMsMMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_bar = 200\n",
    "k_grid = np.arange(1,k_bar) # The range of k's to consider\n",
    "SSE = np.zeros(k_bar) \n",
    "\n",
    "for k in range(k_bar):\n",
    "    fitted_model = KNeighborsRegressor(n_neighbors=k+1).fit(X_train,Y_train) \n",
    "    Y_hat = fitted_model.predict(X_test) # Predict values for test set\n",
    "    SSE[k] = np.sum( (Y_test-Y_hat)**2 ) # Save the computed SSE\n",
    " \n",
    "SSE_min = np.min(SSE) # Lowest recorded SSE\n",
    "min_index = np.where(SSE==SSE_min) # Find the indices of y that equal the minimum\n",
    "k_star = k_grid[min_index] # Find the optimal value of k\n",
    "print(k_star)\n",
    "# Prints [12]\n",
    "\n",
    "plt.plot(np.arange(0,k_bar),SSE) # Plot SSE by k\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"optimal k:\"+str(k_star))\n",
    "plt.ylabel('SSE')\n",
    "plt.show()\n",
    "# 12 is the lowest point on the y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80343f58-d044-4841-89aa-f35d4b65bf1c",
   "metadata": {},
   "source": [
    "The optimal k is around 12. Therefore, k values of 25 and higher will probably overfit the model as values tend to horizontally bunch. K values of 3 and 10 will result in underfitting of the model, with high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b8586",
   "metadata": {},
   "source": [
    "**Q2.** This is a case study on $k$ nearest neighbor classification, using the `animals.csv` data.\n",
    "\n",
    "The data consist of a label, `class`, taking integer values 1 to 7, the name of the species, `animal`, and 16 characteristics of the animal, including `hair`, `feathers`, `milk`, `eggs`, `airborne`, and so on. \n",
    "\n",
    "1. Load the data. For each of the seven class labels, print the values in the class and get a sense of what is included in that group. Perform some other EDA: How big are the classes? How much variation is there in each of the features/covariates? Which variables do you think will best predict which class?\n",
    "2. Split the data 50/50 into training and test/validation sets. (The smaller the data are, the more equal the split should be, in my experience: Otherwise, all of the members of one class end up in the training or test data, and the model falls apart.)\n",
    "3. Using all of the variables, build a $k$-NN classifier. Explain how you select $k$.\n",
    "4. Print a confusion table for the optimal model, comparing predicted and actual class label on the test set. How accurate it is? Can you interpret why mistakes are made across groups?\n",
    "5. Use only `milk`, `aquatic`, and `airborne` to train a new $k$-NN classifier. Print your confusion table. Mine does not predict all of the classes, only a subset of them. To see the underlying probabilities, use `model.predict_proba(X_test.values)` to predict probabilities rather than labels for your `X_test` test data for your fitted `model`. Are all of the classes represented? Explain your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008c35c",
   "metadata": {},
   "source": [
    "**Q3.** This question is a case study for $k$ nearest neighbor regression, using the `cars_env.csv` data. \n",
    "\n",
    "The variables that go into the model are one more dimension to think about: We can use the train-test split approach to pick the variables that go into the model, not just the number of neighbors.\n",
    "\n",
    "1. Load the data. We're going to use `footprint`, `baseline mpg`, `baseline price`, and `baseline sales`. Prepare some EDA results for these variables: describe tables, histograms/kernel density plots, scatterplots, etc. I renamed these variables to `footprint`, `mpg`, `price`, and `sales` to save time.\n",
    "2. Maxmin normalize `footprint`, `mpg`, and `price`. These will be our features/covariates $X$. Our target/dependent/outcome variable $y$ will be sales. Does it make sense to normalize $y$?\n",
    "3. Make a 30% train-test split of the data into 30% test/validation data and 70% training data. \n",
    "4. Using all three covariates --- `footprint`, `mpg`, and `price` --- what's the best $k$ to use? What SSE is achieved on the test set? To answer these questions, evalute the sum of squared error on the test set for a reasonable range of values of $k$ (perhaps 2 to 150), and find the $k$ with the lowest SSE. \n",
    "5. Do part 4 again, for each pair of variables: `footprint` and `mpg`, `footprint` and `price`, `mpg` and `price`.\n",
    "6. Which set of variables from parts 4 and 5 does the best, in terms of minimizing SSE at the optimal choice of neighbors? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ff5e3",
   "metadata": {},
   "source": [
    "**Q4.** This is a case study on $k$ nearest neighbor classification, using the `land_mines.csv` data.\n",
    "\n",
    "The data consists of a label, `mine_type`, taking integer values 1 to 5, and three properties of the mine, `voltage`, `height` and `soil`. We want to predict the kind of mine from data about it. Imagine working for the DOD or a humanitarian aid agency, trying to help people remove land mines more safely.\n",
    "\n",
    "1. Load the data. Perform some EDA, summarizing the target label and the features.\n",
    "2. Split the sample 50/50 into training and test/validation sets. (The smaller the data are, the more equal the split should be, in my experience: Otherwise, all of the members of one class end up in the training or test data, and the model falls apart.)\n",
    "3. Build a $k$-NN classifier. Explain how you select $k$.\n",
    "4. Print a confusion table for the optimal model, comparing predicted and actual class label on the test set. How accurate is it? Where is performance more or less accurate?\n",
    "5. Notice that you can have a lot of accurate predictions for a given type of mine, but still make a lot of mistakes. Please explain how you'd advise someone to actually use this predictive model in practice, given the errors that it tends to make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b57f7-bf4f-4494-b54c-49c4f3ae3ab9",
   "metadata": {},
   "source": [
    "**Q5.** This question is a case study for $k$ nearest neighbor regression, using the `heart_failure_clinical_records_dataset.csv` data.\n",
    "\n",
    "The data for the question include:\n",
    "\n",
    "- age: age of the patient (years)\n",
    "- anaemia: decrease of red blood cells or hemoglobin (boolean)\n",
    "- high blood pressure: if the patient has hypertension (boolean)\n",
    "- creatinine phosphokinase (CPK): level of the CPK enzyme in the blood (mcg/L)\n",
    "- diabetes: if the patient has diabetes (boolean)\n",
    "- ejection fraction: percentage of blood leaving the heart at each contraction (percentage)\n",
    "- platelets: platelets in the blood (kiloplatelets/mL)\n",
    "- sex: woman or man (binary)\n",
    "- serum creatinine: level of serum creatinine in the blood (mg/dL)\n",
    "- serum sodium: level of serum sodium in the blood (mEq/L)\n",
    "- smoking: if the patient smokes or not (boolean)\n",
    "- time: follow-up period (days)\n",
    "- death event: if the patient deceased during the follow-up period (boolean)\n",
    "\n",
    "1. Load the `./data/heart_failure_clinical_records_dataset.csv`. Are there any `NA`'s to handle? use `.drop()` to remove `time` from the dataframe.\n",
    "2. Make a correlation matrix. What variables are strongly associated with a death event?\n",
    "3. For the dummy variables `anaemia`, `diabetes`, `high_blood_pressure`, `sex`, and `smoking`, compute a summary table of `DEATH_EVENT` grouped by the variable. For which variables does a higher proportion of the population die when the variable takes the value 1 rather than 0?\n",
    "4. On the basis of your answers from 2 and 3, build a matrix $X$ of the variables you think are most predictive of a death, and a variable $y$ equal to `DEATH_EVENT`.\n",
    "5. Maxmin normalize all of the variables in `X`.\n",
    "6. Split the sample into ~80% for training and ~20% for evaluation. (Try to use the same train/test split for the whole question, so that you're comparing apples to apples in the questions below.).\n",
    "7. Determine the optimal number of neighbors for a $k$NN regression for the variables you selected.\n",
    "8. OK, do steps 5 through 7 again, but use all of the variables (except `time`). Which model has a lower Sum of Squared Error? Which would you prefer to use in practice, if you had to predict `DEATH_EVENT`s? If you play with the selection of variables, how much does the SSE change for your fitted model on the test data? Are more variables always better? Explain your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d193de6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Q6.** This is a case study on $k$ nearest neighbor regression and imputation, using the `airbnb_hw.csv` data.\n",
    "\n",
    "There are 30,478 observations, but only 22,155 ratings. We're going to build a kNN regressor to impute missing values. This is a common task, and illustrates one way you can use kNN in the future even when you have more advanced models available.\n",
    "\n",
    "1. Load the `airbnb_hw.csv` data with Pandas. We're only going to use `Review Scores Rating`, `Price`, and `Beds`, so use `.loc` to reduce the dataframe to those variables.\n",
    "2. Set use `.isnull()` to select the subset of the dataframe with missing review values. Set those aside in a different dataframe. We'll make predictions about them later.\n",
    "3. Use `df = df.dropna(axis = 0, how = 'any')` to eliminate any observations with missing values/NA's from the dataframe.\n",
    "4. For the complete cases, create a $k$-NN model that uses the variables `Price` and `Beds` to predict `Review Scores Rating`. How do you choose $k$? (Hint: Train/test split, iterate over reasonable values of $k$ and find a value that minimizes SSE on the test split using predictions from the training set.)\n",
    "5. Predict the missing ratings. \n",
    "6. Do a kernel density plot of the training ratings and the predicted missing ratings. Do they look similar or not? Explain why."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
