











#1. Load the `./data/USA_cars_datasets.csv`. Keep the following variables and drop the rest: `price`, `year`, `mileage`. Are there any `NA`'s to handle? Look at the head and dimensions of the data.

import numpy as np
import pandas as pd

df = pd.read_csv('./data/USA_cars_datasets.csv')
df = df.loc[:,['price','year','mileage'] ]
print(df.shape)
print(df.describe())
df.head()
#> There are no `nan`s, since there are 2499 observations of each variable, and 2499 rows in the dataset.

#2. Maxmin normalize `year` and `mileage`.

def maxmin(x):
    u = (x-min(x))/(max(x)-min(x))
    return u

df['year'] = maxmin(df['year'])
df['mileage'] = maxmin(df['mileage'])


#3. Split the sample into ~80% for training and ~20% for evaluation.

from sklearn.model_selection import train_test_split
y = df['price']
X = df.drop('price',axis=1)
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=100)

#4. Use the $k$NN algorithm and the training data to predict `price` using `year` and `mileage` for the test set for $k=3,10,25,50,100,300$. For each value of $k$, compute the mean squared error and print a scatterplot showing the test value plotted against the predicted value. What patterns do you notice as you increase $k$?

from sklearn.neighbors import KNeighborsRegressor
import matplotlib.pyplot as plt

for k in [3,10,25,50,100,300]:
    model = KNeighborsRegressor(n_neighbors=k).fit(X_train,y_train)
    y_hat = model.predict(X_test)
    SSE = np.sum( (y_test-y_hat)**2 )
    #
    plot, axes = plt.subplots()
    plt.scatter(y_test,y_hat)
    plt.title('k: '+str(k)+', SSE: '+str(SSE))
    axes.set_ylim(-1000, 62000)
    axes.set_xlim(-1000, 62000)
    plt.show()
#> The range of predicted values flattens, and the variability of predictions onditional on price shrinks.

#5. Determine the optimal $k$ for these data.

k_bar = 200
k_grid = np.arange(1,k_bar) # The range of k's to consider
SSE = np.zeros(k_bar) 

for k in range(k_bar):
    fitted_model = KNeighborsRegressor(n_neighbors=k+1).fit(X_train,y_train) 
    y_hat = fitted_model.predict(X_test) # Predict values for test set
    SSE[k] = np.sum( (y_test-y_hat)**2 ) # Save the computed SSE
 
SSE_min = np.min(SSE) # Lowest recorded SSE
min_index = np.where(SSE==SSE_min) # Find the indices of y that equal the minimum
k_star = k_grid[min_index] # Find the optimal value of k
print(k_star)

plt.plot(np.arange(0,k_bar),SSE) # Plot SSE by k
plt.xlabel("k")
plt.title("optimal k:"+str(k_star))
plt.ylabel('SSE')
plt.show()

#6. Describe what happened in the plots of predicted versus actual prices as $k$ varied, taking your answer into part 6 into account. (Hint: Use the words "underfitting" and "overfitting".)

#> The optimal $k$ is around 77, so for 300, the model is probably overfitting, and for 3, 10, and 25, the model is underfitting. For $k$ equal to 50 and 100, the answer is pretty close. What do we notice about $k=300$? There is a bunch of horizontal bunching, where common answers start to exert a lot of influence on the predictions and organize them into horizontal groups. Since the data are fairly evenly distributed, this is an unnatural artifact of over-fitting. On the other hand, for very small $k$, like 3, the predictions are high variance and low precision.

#> The other thing to notice is that as $k$ increases, the range of predictions shrinks: The high, outlier values become less influential, and predictions shrink towards average values. 






import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

#1. Load the data. For each of the seven class labels, print the values in the class and get a sense of what is included in that group. Perform some other EDA: How big are the classes? How much variation is there in each of the features/covariates? Which variables do you think will best predict which class?

df = pd.read_csv('./data/zoo.csv')
print(df.columns)
print(df.shape)
df.head()

classes = list(df['class'].unique())
[ print(df.loc[ df['class'] == item, 'animal'].unique()) for item in classes]

#> There's a large chordate class for 1, fish in 2, birds in 3, miscellaneous sea creatures in 4, bugs in 5, reptiles in 6, and tube creatures in 7.

df['class'].hist(grid=False)
plt.title('Class labels')
plt.show()

df['legs'].hist(grid=False)
plt.title('Number of Legs')
plt.show()

df['catsize'].hist(grid=False)
plt.title('Catsize')
plt.show()

#2. Split the data 50/50 into training and test/validation sets. (The smaller the data are, the more equal the split should be, in my experience: Otherwise, all of the members of one class end up in the training or test data, and the model falls apart.)

## Select data:
x_vars = df.columns.tolist()
y = df['class'] # Target variable
x_vars.remove('animal')
x_vars.remove('class')
X = df.loc[:,x_vars] # Create feature matrix
X.describe()

## Split the sample:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables
                                                    test_size=.5,
                                                    random_state=125) # For replication purposes

#3. Using all of the variables, build a $k$-NN classifier. Explain how you select $k$.

from sklearn.neighbors import KNeighborsClassifier

## Solve for k that maximizes accuracy:
N_train = len(y_train)
N_test = len(y_test)
k_bar = N_train-1 # Number of k's to try
Acc = np.zeros(k_bar) # We'll store the accuracy here 

for k in range(k_bar):
    model = KNeighborsClassifier(n_neighbors=k+1) # Create a sk model for k
    fitted_model = model.fit(X_train.values,y_train) # Train the model on our data
    y_hat = fitted_model.predict(X_test.values) # Predict values for test set
    Acc[k] = np.sum( y_hat == y_test )/N_test # Accuracy on testing data

Acc_max = np.max(Acc) # Find highest recorded Accuracy
max_index = np.where(Acc==Acc_max) # Find the indices that equal the maximum
if len(max_index) > 1:
    print(max_index[len(max_index)])
    max_index = max_index[len(max_index)]
k_star = max_index[0]+1 # Find the optimal value of k; why index+1?
print(Acc_max)
print(k_star)

## Accuracy plot:
plt.plot(np.arange(0,k_bar),Acc,label='Accuracy') 
plt.xlabel("k")
plt.ylabel("Accuracy")
plt.legend(loc='upper right')
plt.title('Test Accuracy')
plt.show()

#4. Print a confusion table for the optimal model, comparing predicted and actual class label on the test set. How accurate it is? Can you interpret why mistakes are made across groups?

## Fit optimal model:
model = KNeighborsClassifier(n_neighbors=k_star[0]) # Create a sk model for k
fitted_model = model.fit(X_train.values,y_train) # Train the model on our data
y_hat = fitted_model.predict(X_test.values) # Predict values for test set

## Confusion Table:
print('All variables, confusion table:', pd.crosstab(y_hat, y_test))


#5. Use only `milk`, `aquatic`, and `airborne` to train a new $k$-NN classifier. Print your confusion table. Mine does not predict all of the classes, only a subset of them. To see the underlying probabilities, use `model.predict_proba(X_test.values)` to predict probabilities rather than labels for your `X_test` test data for your fitted `model`. Are all of the classes represented? Explain your results.

## Select data:
x_vars = ['milk','aquatic','airborne']

y = df['class'] # Target variable
X = df.loc[:,x_vars] # Create feature matrix

## Split the sample:
X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables
                                                    test_size=.5)#, # Split the sample 80 train/ 20 test
                                                    #random_state=200) # For replication purposes
N_train = len(y_train)
N_test = len(y_test)

## Solve for k that maximizes accuracy:
k_bar = N_train # Number of k's to try
Acc = np.zeros(k_bar) # We'll store the accuracy here 

for k in range(k_bar):
    model = KNeighborsClassifier(n_neighbors=k+1) # Create a sk model for k
    fitted_model = model.fit(X_train.values,y_train) # Train the model on our data
    y_hat = fitted_model.predict(X_test.values) # Predict values for test set
    Acc[k] = np.sum( y_hat == y_test )/N_test # Accuracy on testing data

Acc_max = np.max(Acc) # Find highest recorded Accuracy
max_index = np.where(Acc==Acc_max) # Find the indices that equal the maximum
if len(max_index) > 1:
    print(max_index[len(max_index)])
    max_index = max_index[len(max_index)]
k_star = max_index[0]+1 # Find the optimal value of k; why index+1?
print(Acc_max)
print(k_star)


## Accuracy plot:
plt.plot(np.arange(0,k_bar),Acc,label='Accuracy') 
plt.xlabel("k")
plt.ylabel("Accuracy")
plt.legend(loc='upper right')
plt.title('Test Accuracy')
plt.show()

## Fit optimal model:
model = KNeighborsClassifier(n_neighbors=k_star[0]) # Create a sk model for k
fitted_model = model.fit(X_train.values,y_train) # Train the model on our data
y_hat = model.predict(X_test.values) # Predict values for test set

## Confusion Table:
print('Confusion table:', pd.crosstab(y_hat, y_test))

print('Notice how many classes are empty in the confusion table.')

print( model.predict_proba(X_test.values) )

print('Notice how that, despite those classes being empty, they have non-zero predictive probability.')
print('This is an example of why making hard label classifications can be risky:')
print('you throw away information about the prevalence of "minority classes" that do not have a high likelihood.')





# 1. Load the data. We're going to use `footprint`, `baseline mpg`, `baseline price`, and `baseline sales`. Prepare some EDA results for these variables: describe tables, histograms/kernel density plots, scatterplots, etc. I renamed these variables to `footprint`, `mpg`, `price`, and `sales` to save time.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df = pd.read_csv('./data/cars_env.csv')
print(df.head())

y = df['baseline sales']
X = df.loc[:, ['footprint','baseline mpg','baseline price'] ]

X = X.rename(columns = {'baseline mpg':'mpg','baseline price':'price'})
print(X.head())

# 2. Maxmin normalize `footprint`, `mpg`, and `price`. These will be our features/covariates $X$. Our target/dependent/outcome variable $y$ will be sales. Does it make sense to normalize $y$?

def maxmin(z):
    u = (z-min(z))/(max(z)-min(z))
    return u
X = X.apply(maxmin)
X.describe()

# 3. Make a 30% train-test split of the data into 30% test/validation data and 70% training data. 

from sklearn.model_selection import train_test_split # Split the sample
X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables
                                                    test_size=.3, # Split the sample 80 train/ 20 test
                                                    random_state=300) # For replication purposes

# 4. Using all three covariates --- `footprint`, `mpg`, and `price` --- what's the best $k$ to use? What SSE is achieved on the test set? To answer these questions, evalute the sum of squared error on the test set for a reasonable range of values of $k$ (perhaps 2 to 150), and find the $k$ with the lowest SSE. 

from sklearn.neighbors import KNeighborsRegressor # Import the kNN regression tool

## Price, mpg, and footprint:
k_bar = 200 # Number of k's to try
SSE = np.zeros(k_bar) # We'll store the SSE here 
SSE_on_train = np.zeros(k_bar) # For pedogogical purposes, we'll save the training error

for k in range(k_bar):
    model = KNeighborsRegressor(n_neighbors=k+1) # Create a sk model for k
    fitted_model = model.fit(X_train,y_train) # Train the model on our data
    y_hat = fitted_model.predict(X_test) # Predict values for test set
    SSE[k] = np.sum( (y_test-y_hat)**2 ) # Save the computed SSE for test set
    y_hat = fitted_model.predict(X_train) # Predict values for training set
    SSE_on_train[k] = np.sum( (y_train-y_hat)**2 ) # Save the computed SSE

SSE_min = np.min(SSE) # Find lowest recorded SSE
min_index = np.where(SSE==SSE_min) # Find the indices of SSE that equal the minimum
k_star = min_index[0]+1 # Find the optimal value of k; why index+1?
print(k_star)

plt.plot(np.arange(0,k_bar),SSE,label='Test SSE') # Plot SSE by k
plt.xlabel("k")
plt.ylabel("SSE")
plt.legend(loc='upper right')
plt.title('SSE')
plt.show()

# 5. Do part 4 again, for each pair of variables: `footprint` and `mpg`, `footprint` and `price`, `mpg` and `price`.

X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables
                                                    test_size=.4, # Split the sample 80 train/ 20 test
                                                    random_state=10000) # For replication purposes

vars1 = ['footprint','price']
vars2 = ['mpg','price']
vars3 = ['mpg','footprint']
vars4 = ['mpg','footprint', 'price']

kgrid = np.arange(2,150)
N = len(y)

SSE_results = []
y_hat_results = []
for var_list in [vars1, vars2, vars3,vars4]:
    X_train_1 = X_train.loc[:,var_list ]
    X_test_1 = X_test.loc[:,var_list]
    SSE = np.zeros(len(kgrid)) # We'll store the SSE here 
    for k in range(len(kgrid)):
        model = KNeighborsRegressor(n_neighbors=kgrid[k]) # Create a sk model for k
        fitted_model = model.fit(X_train_1,y_train) # Train the model on our data
        y_hat = fitted_model.predict(X_test_1) # Predict values for test set
        y_hat_results.append(y_hat)
        SSE_results.append( {'sse':np.sum( (y_test-y_hat)**2)/N,
                            'vars':str(var_list),
                            'k':k} ) # Save the computed SSE for test set
gdf = pd.DataFrame.from_dict(SSE_results)

sns.lineplot(y='sse',x='k',data=gdf,hue='vars')
plt.show()

print( np.min( gdf.loc[gdf['vars'] == str(vars1),'sse']))
print( np.min( gdf.loc[gdf['vars'] == str(vars2),'sse']))
print( np.min( gdf.loc[gdf['vars'] == str(vars3),'sse']))
print( np.min( gdf.loc[gdf['vars'] == str(vars4),'sse']))

# 6. Which set of variables from parts 4 and 5 does the best, in terms of minimizing SSE at the optimal choice of neighbors? Explain.

print('Footprint and price appear to do the best, in terms of predicting consumer behavior (i.e. sales).')






# 1. Load the data. Perform some EDA, summarizing the target label and the features.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

df = pd.read_csv('./data/land_mines.csv')

print(df.shape)
print(df.head())
print(df.describe())

df['voltage'].hist(grid=False)
plt.title('Voltage')
plt.show()

df['height'].hist(grid=False)
plt.title('Height')
plt.show()

df['soil'].hist(grid=False)
plt.title('Soil')
plt.show()

df['mine_type'].hist(grid=False)
plt.title('Type')
plt.show()

sns.pairplot(df, hue="mine_type", palette='crest')

# 2. Split the sample 50/50 into training and test/validation sets. (The smaller the data are, the more equal the split should be, in my experience: Otherwise, all of the members of one class end up in the training or test data, and the model falls apart.)

## Split the sample:
y = df['mine_type'] # Target variable
X = df.loc[:,['voltage','height','soil']] # Create feature matrix

X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables
                                                    test_size=.5, # Split the sample 80 train/ 20 test
                                                    random_state=200) # For replication purposes

# 3. Build a $k$-NN classifier. Explain how you select $k$.
N_train = len(y_train)
N_test = len(y_test)

## Solve for k that maximizes accuracy:
k_bar = N_train # Number of k's to try
Acc = np.zeros(k_bar) # We'll store the accuracy here 

for k in range(k_bar):
    model = KNeighborsClassifier(n_neighbors=k+1) # Create a sk model for k
    fitted_model = model.fit(X_train.values,y_train) # Train the model on our data
    y_hat = fitted_model.predict(X_test.values) # Predict values for test set
    Acc[k] = np.sum( y_hat == y_test )/N_test # Accuracy on testing data
Acc_max = np.max(Acc) # Find highest recorded Accuracy
max_index = np.where(Acc==Acc_max) # Find the indices that equal the maximum
k_star = max_index[0]+1 # Find the optimal value of k; why index+1?
print('Maximum accuracy: ', Acc_max)
print('Optimal k: ', k_star)


## Accuracy plot:
plt.plot(np.arange(0,k_bar),Acc,label='Accuracy') 
plt.xlabel("k")
plt.ylabel("Accuracy")
plt.legend(loc='upper right')
plt.title('Test Accuracy')
plt.show()

## Fit optimal model:
model = KNeighborsClassifier(n_neighbors=k_star[0]) # Create a sk model for k
fitted_model = model.fit(X_train.values,y_train) # Train the model on our data
y_hat = fitted_model.predict(X_test.values) # Predict values for test set


# 4. Print a confusion table for the optimal model, comparing predicted and actual class label on the test set. How accurate is it? Where is performance more or less accurate?

## Confusion Table:
print(pd.crosstab(y_hat, y_test))

print('The accuracy is around 40%, which is probably not something you want to risk lives or limbs on.')
print('When 1 is predicted, it is often 1, but often 3,4,5 as well.')
print('When 2 is predicted, the prediction is pretty reliable.')
print('When 3 or 4 are predicted, it could be any of the types.')
print('When 5 is predicted, the results are bad.')

# 5. Notice that you can have a lot of accurate predictions for a given type of mine, but still make a lot of mistakes. Please explain how you'd advise someone to actually use this predictive model in practice, given the errors that it tends to make.

print('For this model, I might use it to predict 2, but otherwise there are')
print('a lot of errors that make it risky to use in practice.')






# 1. Load the `./data/heart_failure_clinical_records_dataset.csv`. Are there any `NA`'s to handle? use `.drop()` to remove `time` from the dataframe.

import pandas as pd
import numpy as np
df = pd.read_csv('./data/heart_failure_clinical_records_dataset.csv')
print(df.shape)
print(df.describe())

#> No missing values, since 299 values for every variable and 299 observations in total.

df = df.drop('time',axis=1)

# 2. Make a correlation matrix. What variables are strongly associated with a death event?
print(df.corr())
print('The variables with the strongest correlation with `DEATH_EVENT` are age (.254), ejection_fraction (-.269), and serum_creatine (.294). ')

# 3. For the dummy variables `anaemia`, `diabetes`, `high_blood_pressure`, `sex`, and `smoking`, compute a summary table of `DEATH_EVENT` grouped by the variable. For which variables does a higher proportion of the population die when the variable takes the value 1 rather than 0?
vars = ['anaemia','diabetes','high_blood_pressure','sex','smoking']
for var in vars:
    print(df.loc[:,[var,'DEATH_EVENT']].groupby(var).describe())
    
#> > Let's look at the means for high_blood_pressure. For the proportion of the population that has HBP, the DEATH_EVENT average is .371, while for the proportion of the population that does not have HBP, the DEATH_EVENT average is only .294. That's a 27% increase in the frequency of death events. So HPB seems highly predictive. On the other hand, for sex, the mean values are almost the same for men and women, at .32, so the sex variable isn't a very powerful predictor of death events. Anaemia and high blood pressure seem like the strongest predictors.



# 4. On the basis of your answers from 2 and 3, build a matrix $X$ of the variables you think are most predictive of a death, and a variable $y$ equal to `DEATH_EVENT`.

y = df['DEATH_EVENT']
vars = ['age','ejection_fraction','serum_creatinine','high_blood_pressure','anaemia']
X = df.loc[:,vars]

# 5. Maxmin normalize all of the variables in `X`.

def maxmin(x):
    u = (x-min(x))/(max(x)-min(x))
    return u
X = X.apply(maxmin)

# 6. Split the sample into ~80% for training and ~20% for evaluation. (Try to use the same train/test split for the whole question, so that you're comparing apples to apples in the questions below.).

#from sklearn.model_selection import train_test_split
#X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.2,random_state=100)
np.random.seed(100) 
N = X.shape[0]
all = np.arange(1,N)
train = np.random.choice(N,int(.8*N) ) # Generate random indices for training set
test = [item for item in all if item not in train] # Find test indices

#test = np.where( train not in np.linspace(1,N) )
X_train = X.iloc[train,:]
y_train = y.iloc[train]
X_test = X.iloc[test,:]
y_test = y.iloc[test]

# 7. Determine the optimal number of neighbors for a $k$NN regression for the variables you selected.

from sklearn.neighbors import KNeighborsRegressor

# Determine the optimal k:
k_bar = 25
k_grid = np.arange(1,k_bar) # The range of k's to consider
SSE = np.zeros(k_bar) 

for k in range(k_bar):
    knn = KNeighborsRegressor(n_neighbors=k+1)
    predictor = knn.fit(X_train,y_train) 
    y_hat = knn.predict(X_test)
    SSE[k] = np.sum( (y_test-y_hat)**2 ) # Bug in sklearn requires .values

SSE_min = np.min(SSE) # highest recorded accuracy
min_index = np.where(SSE==SSE_min) 
k_star = k_grid[min_index] # Find the optimal value of k
print(k_star)

plt.plot(np.arange(0,k_bar),SSE) # Plot accuracy by k
plt.xlabel("k")
plt.title("optimal k:"+str(k_star)+', SSE:'+str(SSE_min))
plt.ylabel('SSE')
plt.show()

# 8. OK, do steps 5 through 7 again, but use all of the variables (except `time`). Which model has a lower Sum of Squared Error? Which would you prefer to use in practice, if you had to predict `DEATH_EVENT`s? If you play with the selection of variables, how much does the SSE change for your fitted model on the test data? Are more variables always better? Explain your findings.

X = df.drop('DEATH_EVENT',axis=1)

X_train = X.iloc[train,:]
y_train = y.iloc[train]
X_test = X.iloc[test,:]
y_test = y.iloc[test]

from sklearn.neighbors import KNeighborsRegressor

# Determine the optimal k:
k_bar = 100
k_grid = np.arange(1,k_bar) # The range of k's to consider
SSE = np.zeros(k_bar) 

for k in range(k_bar):
    knn = KNeighborsRegressor(n_neighbors=k+1)
    predictor = knn.fit(X_train,y_train) 
    y_hat = knn.predict(X_test)
    SSE[k] = np.sum( (y_test-y_hat)**2 ) # Bug in sklearn requires .values

SSE_min = np.min(SSE) # highest recorded accuracy
min_index = np.where(SSE==SSE_min) 
k_star = k_grid[min_index] # Find the optimal value of k
print(k_star)

plt.plot(np.arange(0,k_bar),SSE) # Plot accuracy by k
plt.xlabel("k")
plt.title("optimal k:"+str(k_star)+', SSE:'+str(SSE_min))
plt.ylabel('SSE')
plt.show()

# >> With more variables, the algorithm selects a higher optimal $k^* = 83$ instead of $k^*=6$, and it has a higher SSE of 29 rather than 27. The simpler model (fewer variables, fewer neighbors) does a better job predicting.






# 1. Load the `airbnb_hw.csv` data with Pandas. We're only going to use `Review Scores Rating`, `Price`, and `Beds`, so use `.loc` to reduce the dataframe to those variables.

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

df = pd.read_csv('./data/airbnb_hw.csv')
df = df.loc[:,['Review Scores Rating','Price','Beds'] ]
print(df.head())

print(df.describe())

df['Price'] = df['Price'].str.replace(',','')
df['Price'] = pd.to_numeric( df['Price'], errors='coerce')

print(df.describe())
print(df.dtypes)

df = df.rename(columns={'Price':'price','Beds':'beds','Review Scores Rating':'review'})


# 2. Set use `.isnull()` to select the subset of the dataframe with missing review values. Set those aside in a different dataframe. We'll make predictions about them later.
select = df['review'].isnull()
df_to_impute = df.loc[select,:]

# 3. Use `df = df.dropna(axis = 0, how = 'any')` to eliminate any observations with missing values/NA's from the dataframe.
df = df.dropna(axis=0,how='any')


# 4. For the complete cases, create a $k$-NN model that uses the variables `Price` and `Beds` to predict `Review Scores Rating`. How do you choose $k$? (Hint: Train/test split, iterate over reasonable values of $k$ and find a value that minimizes SSE on the test split using predictions from the training set.)

y = df['review']
X = df.loc[:,['price','beds'] ]

## Split the sample:
from sklearn.neighbors import KNeighborsRegressor # Import the kNN regression tool
from sklearn.model_selection import train_test_split # Split the sample
X_train, X_test, y_train, y_test = train_test_split(X,y, # Feature and target variables
                                                    test_size=.3, # Split the sample 80 train/ 20 test
                                                    random_state=300) # For replication purposes

k_bar = 200 # Number of k's to try
SSE = np.zeros(k_bar) # We'll store the SSE here 

for k in range(k_bar):
    model = KNeighborsRegressor(n_neighbors=k+1) # Create a sk model for k
    fitted_model = model.fit(X_train,y_train) # Train the model on our data
    y_hat = fitted_model.predict(X_test) # Predict values for test set
    SSE[k] = np.sum( (y_test-y_hat)**2 ) # Save the computed SSE for test set
    y_hat = fitted_model.predict(X_train) # Predict values for training set

SSE_min = np.min(SSE) # Find lowest recorded SSE
min_index = np.where(SSE==SSE_min) # Find the indices of SSE that equal the minimum
k_star = min_index[0]+1 # Find the optimal value of k; why index+1?
print(k_star)

plt.plot(np.arange(0,k_bar),SSE,label='Test SSE') # Plot SSE by k
plt.xlabel("k")
plt.ylabel("SSE")
plt.legend(loc='upper right')
plt.title('SSE')
plt.show()

# Fit the optimal model:
model = KNeighborsRegressor(n_neighbors=k_star[0]) # Create a sk model for k
fitted_model = model.fit(X_train,y_train) # Train the model on our data

# 5. Predict the missing ratings. 
x_impute = df_to_impute.loc[:,['price','beds'] ]
x_impute = x_impute.dropna(axis=0,how='any')
y_impute = fitted_model.predict(x_impute)

# 6. (Optional) Do a kernel density plot of the training ratings and the predicted missing ratings. Do they look similar or not? Explain why.

imputed = pd.DataFrame.from_dict({ 'y':y_impute,'group':'imputed'})
true = pd.DataFrame.from_dict({'y':df['review'],'group':'actual'})
gdf = pd.concat([imputed,true],axis=0)

sns.kdeplot(x='y',hue='group',data=gdf)
plt.title('Imputed versus Actual Values')

print('The imputed values are much more compressed than the real ones.')




